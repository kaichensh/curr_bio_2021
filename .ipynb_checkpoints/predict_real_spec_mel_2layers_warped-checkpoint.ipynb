{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gpus = [3]\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import scipy.io as sio\n",
    "import socket\n",
    "import sys\n",
    "import h5py\n",
    "import keras\n",
    "import random\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM, Activation, advanced_activations\n",
    "from keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import spec_processing as spp\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "from scipy.signal import spectrogram\n",
    "import scipy.interpolate\n",
    "import logging\n",
    "import copy\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from random import shuffle\n",
    "import IPython.display\n",
    "import matplotlib\n",
    "matplotlib.rcParams['pdf.fonttype'] = 42\n",
    "matplotlib.rcParams['ps.fonttype'] = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date and bird of data used\n",
    "bird_id = 'z007'\n",
    "model_name = 'spectrogram prediction model/mel'\n",
    "\n",
    "#locate source folder\n",
    "bird_folder_data, bird_folder_save, repos_folder, results_folder = spp.locate_folders(bird_id, model_name)\n",
    "    \n",
    "print('Bird data located at:\\n'+bird_folder_data+'\\n')\n",
    "print('Results will be saved at:\\n'+results_folder+'\\n')\n",
    "\n",
    "output_file = os.path.join(results_folder, 'mel_spec_predicted.dat')\n",
    "\n",
    "sys.path.append(os.path.join(repos_folder, 'swissknife'))\n",
    "\n",
    "from dynamo import finch as zf\n",
    "from dynamo import takens as tk\n",
    "from bci.core import expstruct as es\n",
    "from bci import synthetic as syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "specify parameters and data folder to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_file_name = 'experiment.kwik'\n",
    "song_file_name = 'experiment.sng.kwe'\n",
    "song_length = 20480\n",
    "bin_size = 128\n",
    "##\n",
    "num_clusters = 64\n",
    "num_lookbacks = 10\n",
    "\n",
    "start_extend_bins = 0\n",
    "end_extend_bins = 0\n",
    "n_mel_freq = 64\n",
    "plot_raster = True\n",
    "slice_shuffle_pattern = None\n",
    "warp_method = 'anderson'\n",
    "session = 'day-2016-09-10'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "slice_shuffle_pattern dictates whether spectrogram slices will be shuffled based on a random pattern, defaults to False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(spp)\n",
    "if slice_shuffle_pattern and not isinstance(slice_shuffle_pattern, list):\n",
    "    datasets, generated_pattern = spp.make_datasets_finch(neural_file_name, song_file_name, song_length, bin_size, num_clusters, num_lookbacks, \n",
    "                                        bird_id, model_name, start_extend_bins=start_extend_bins, end_extend_bins=end_extend_bins,\n",
    "                                        specify_subdir = session, n_mel_freq_components = n_mel_freq, \n",
    "                                       slice_shuffle_pattern =slice_shuffle_pattern, warp_method = warp_method)\n",
    "    slice_shuffle_pattern = generated_pattern[:]\n",
    "else:\n",
    "    datasets = spp.make_datasets_finch(neural_file_name, song_file_name, song_length, bin_size, num_clusters, num_lookbacks, \n",
    "                                        bird_id, model_name, start_extend_bins=start_extend_bins, end_extend_bins=end_extend_bins,\n",
    "                                        specify_subdir = session, n_mel_freq_components = n_mel_freq,\n",
    "                                      slice_shuffle_pattern =slice_shuffle_pattern, warp_method = warp_method)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(dataset, num_bins, divisor=4, break_song=False, test_index=3, cross_valid=False):\n",
    "    '''\n",
    "    Given a dataset, divide the entire set a certain number of parts, use one part for testing and the rest for training\n",
    "    dataset: list, shape (n_songs * n_bins_per_song, 2) where 2 corresponds to input and target arrays\n",
    "    num_bins: int, number of bins per song\n",
    "    divisor: int, number of parts the entire dataset will be divded into\n",
    "    break_song: bool, whether to break songs during training (i.e. piece-wise training vs song-wise training)\n",
    "    test_index: within the divided parts, which part is used for testing\n",
    "    cross_valid: bool, whether to generate cross validation dataset\n",
    "    '''\n",
    "    \n",
    "    if test_index>divisor-1 or type(test_index)!=int:\n",
    "        raise ValueError('test_index should be an integer less than or equal to 3.')\n",
    "    if type(divisor)!=int or divisor<1:\n",
    "        raise ValueError('divisor should be an integer greater than or equal to 1.')\n",
    "       \n",
    "    valid_index = test_index-1\n",
    "    \n",
    "    if valid_index<0:\n",
    "        valid_index = divisor-1\n",
    "    \n",
    "    num_songs = int(len(dataset)/int(num_bins))\n",
    "    \n",
    "    #print('Total number of songs loaded:'+str(num_songs))\n",
    "    \n",
    "    if break_song:\n",
    "        \n",
    "        #break songs into roughly equal parts\n",
    "        num_bins_each_part = spp.divide_parts(num_bins, divisor)\n",
    "        start_indeces, end_indeces = spp.find_start_end_index(num_bins_each_part)\n",
    "        \n",
    "        num_current_test = num_bins_each_part[test_index]\n",
    "        num_current_valid = num_bins_each_part[valid_index]*cross_valid\n",
    "        num_current_train = num_bins-num_current_test-num_current_valid\n",
    "        \n",
    "        #print('Breaking up each song into '+str(num_current_train)+' training points and '+str(num_current_test)+' testing points.')\n",
    "        #print('Currently testing on part No.'+str(test_index+1)+' of the '+ str(divisor)+' parts.')\n",
    "        \n",
    "        train_list = list()\n",
    "        test_list = list()\n",
    "        valid_list = list()\n",
    "        temp_test = list()\n",
    "        temp_train = list()\n",
    "        temp_valid = list()\n",
    "        \n",
    "        for i in range(num_songs):\n",
    "            this_song = dataset[i*num_bins:(i+1)*num_bins]\n",
    "            bins_by_part = list()\n",
    "            for j in range(len(num_bins_each_part)):\n",
    "                start_index = start_indeces[j]\n",
    "                end_index = end_indeces[j]\n",
    "                if test_index == j:\n",
    "                    temp_test = temp_test+this_song[start_index:end_index]\n",
    "                elif valid_index == j and cross_valid:\n",
    "                    temp_valid = temp_valid+this_song[start_index:end_index]\n",
    "                else:\n",
    "                    temp_train = temp_train+this_song[start_index:end_index]\n",
    "                    \n",
    "        #separate neural and song data\n",
    "        for i in range(len(dataset[0])):            \n",
    "            train_list.append([train_set[i] for train_set in temp_train])\n",
    "            test_list.append([test_set[i] for test_set in temp_test])\n",
    "            valid_list.append([valid_set[i] for valid_set in temp_valid])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        num_songs_each_part = spp.divide_parts(num_songs, divisor)\n",
    "        num_test = num_songs_each_part[test_index]*num_bins\n",
    "        num_valid = num_songs_each_part[valid_index]*num_bins\n",
    "        num_train = len(datasets)-num_test-num_valid*cross_valid\n",
    "        \n",
    "        start_indeces, end_indeces = spp.find_start_end_index(num_songs_each_part)\n",
    "        \n",
    "    \n",
    "        print('Not breaking up songs. There are '+str(num_train)+' training sets, and '+str(num_test)+' testing sets.\\n')\n",
    "    \n",
    "        train_list = list()\n",
    "        test_list = list()\n",
    "        valid_list = list()\n",
    "        temp_test = list()\n",
    "        temp_train = list()\n",
    "        temp_valid = list()\n",
    "        \n",
    "        #separate neural and song data\n",
    "        for j in range(divisor):\n",
    "            start_index = start_indeces[j]*num_bins\n",
    "            end_index = end_indeces[j]*num_bins\n",
    "            \n",
    "            if test_index == j:\n",
    "                temp_test = temp_test+datasets[start_index:end_index]\n",
    "            elif valid_index == j and cross_valid:\n",
    "                temp_valid = temp_valid+datasets[start_index:end_index]\n",
    "            else:\n",
    "                temp_train = temp_train+datasets[start_index:end_index]\n",
    "                \n",
    "        for i in range(len(dataset[0])):\n",
    "            train_list.append([train_set[i] for train_set in temp_train])\n",
    "            test_list.append([test_set[i] for test_set in temp_test])\n",
    "            valid_list.append([valid_set[i] for valid_set in temp_valid])\n",
    "        \n",
    "    return train_list, test_list, valid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_song = False\n",
    "num_bins = song_length//bin_size+(end_extend_bins-start_extend_bins)*16-2\n",
    "print(num_bins)\n",
    "divisor= 10\n",
    "l2_val = 0.001\n",
    "dropout_val = 0.2\n",
    "cross_valid = False\n",
    "num_neurons = [20, 30]\n",
    "num_ep = 1000\n",
    "early_stopping = True\n",
    "patience = 50\n",
    "valid_split = 0.1\n",
    "test4valid = False\n",
    "batch_size = 10\n",
    "num_songs = len(datasets)//num_bins\n",
    "if len(datasets)%num_bins:\n",
    "    raise ValueError('number of songs is not integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_indeces = spp.assign_test_indeces(divisor, break_song=break_song)\n",
    "#del model\n",
    "#test_indeces = [0]\n",
    "test_indeces = range(divisor)\n",
    "print(test_indeces)\n",
    "\n",
    "num_bins_per_part = spp.divide_parts(num_bins, divisor)\n",
    "start_index, end_index = spp.find_start_end_index(num_bins_per_part)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_output_compiled = list()\n",
    "test_spec_compiled = list()\n",
    "history_compiled = list()\n",
    "\n",
    "save_name = '[%d_%d_%02d]_%02d_%02d_[%s_%.2f]_[%03d_%03d]_%.3f_%03dep_%s' %(start_extend_bins, end_extend_bins, num_lookbacks, divisor, \n",
    "                                                            test_indeces[0], cross_valid, valid_split, num_neurons[0], \n",
    "                                                            num_neurons[1], l2_val, num_ep, break_song)\n",
    "#save_name example:[2_2_10]_10_7_[False_0.10]_[20_30]_0_2ep_True\n",
    "#run_folder = os.path.join(results_folder, 'test_only')\n",
    "run_folder = os.path.join(results_folder, '{:%Y_%m_%d_%H_%M_%S}'.format(datetime.datetime.now()))\n",
    "if not os.path.exists(run_folder):\n",
    "    os.makedirs(run_folder)\n",
    "\n",
    "par_dict = {'song_length':song_length,\n",
    "             'bin_size':bin_size, \n",
    "             'num_clusters':num_clusters, \n",
    "             'num_lookbacks':num_lookbacks, \n",
    "             'start_extend_bins':start_extend_bins, \n",
    "             'end_extend_bins':end_extend_bins, \n",
    "             'n_mel_freq':n_mel_freq, \n",
    "             'slice_shuffle_pattern':slice_shuffle_pattern,\n",
    "            'break_song':break_song,\n",
    "            'divisor':divisor,\n",
    "            'l2_val':l2_val,\n",
    "            'patience':patience,\n",
    "            'dropout_val':dropout_val,\n",
    "            'cross_valid':cross_valid,\n",
    "            'num_neurons':num_neurons,\n",
    "            'num_ep':num_ep,\n",
    "            'early_stopping':early_stopping,\n",
    "            'valid_split':valid_split,\n",
    "            'test4valid':test4valid,\n",
    "            'batch_size':batch_size,\n",
    "            'warp_method':warp_method,\n",
    "            'test_indeces':test_indeces,\n",
    "            'session':session}\n",
    "\n",
    "dict_file = os.path.join(run_folder, 'parameters.npy')\n",
    "np.save(dict_file, par_dict) \n",
    "    \n",
    "if early_stopping and not (cross_valid or valid_split or test4valid):\n",
    "    raise ValueError('if early stopping, you need to have validation sets.')\n",
    "    \n",
    "for test_index in test_indeces:\n",
    "    \n",
    "    if break_song:\n",
    "        print('Starting preparing for No.'+ str(test_index+1)+ ' of '+str(divisor)+' parts.')\n",
    "        \n",
    "    train_list, test_list, valid_list = make_train_test(datasets, num_bins, divisor=divisor, \n",
    "                                                        break_song=break_song, test_index=test_index, \n",
    "                                                        cross_valid=cross_valid)\n",
    "    #both lists [neural data, on/off, beta, alpha]\n",
    "    train_neuro = np.array(train_list[0])\n",
    "    test_neuro = np.array(test_list[0])    \n",
    "\n",
    "    train_spec = np.array(train_list[1])\n",
    "    test_spec = np.array(test_list[1])\n",
    "\n",
    "    #train_neuro = scaler.fit_transform(train_neuro)\n",
    "    #test_neuro = scaler.fit_transform(test_neuro)\n",
    "\n",
    "    train_neuro = np.reshape(train_neuro, (train_neuro.shape[0], num_lookbacks, num_clusters))\n",
    "    test_neuro = np.reshape(test_neuro, (test_neuro.shape[0], num_lookbacks, num_clusters))\n",
    "    \n",
    "    if cross_valid:\n",
    "        valid_neuro = np.array(valid_list[0])\n",
    "        valid_spec = np.array(valid_list[1])\n",
    "        valid_neuro = np.reshape(valid_neuro, (valid_neuro.shape[0], num_lookbacks, num_clusters))\n",
    "    \n",
    "    #train_neuro = np.reshape(train_neuro, (train_neuro.shape[0], 1, train_neuro.shape[1]))\n",
    "    #test_neuro = np.reshape(test_neuro, (test_neuro.shape[0], 1, test_neuro.shape[1]))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #num_lookback = len(datasets[0][0])//num_channels\n",
    "    #print('The number of lookbacks in this model is '+ str(num_lookbacks) + '\\n')\n",
    "    num_bins_per_part = spp.divide_parts(num_bins, divisor)\n",
    "\n",
    "\n",
    "    model.add(LSTM(num_neurons[0], input_shape = (num_lookbacks, num_clusters),return_sequences=True, W_regularizer=l2(l2_val)))\n",
    "    #model1.add(LSTM(10, input_dim = num_lookback*num_channels,return_sequences=True))\n",
    "    model.add(Dropout(dropout_val))\n",
    "    model.add(LSTM(num_neurons[1], W_regularizer=l2(l2_val)))\n",
    "    model.add(Dropout(dropout_val))\n",
    "    model.add(Dense(len(datasets[0][1]), W_regularizer=l2(l2_val)))\n",
    "\n",
    "    print('Model building finished.')\n",
    "    \n",
    "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "    \n",
    "    current_ep_count=0\n",
    "    \n",
    "    if early_stopping:\n",
    "        model_file = os.path.join(run_folder, '%02d_weights-improvement.h5' %(test_index))\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=patience, verbose=1),\n",
    "                     ModelCheckpoint(filepath=model_file, monitor='val_loss', save_best_only=True, verbose=0)\n",
    "                    ]\n",
    "        if valid_split:\n",
    "            history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(test_indeces)==1), \n",
    "                      validation_split=valid_split, callbacks=callbacks)\n",
    "        elif test4valid:\n",
    "            history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(test_indeces)==1), \n",
    "                      validation_data=(test_neuro, test_spec), callbacks=callbacks)\n",
    "        else:\n",
    "            history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(test_indeces)==1), \n",
    "                      validation_data=(valid_neuro, valid_spec), callbacks=callbacks)\n",
    "        del model\n",
    "        model = load_model(model_file)\n",
    "    \n",
    "    else:\n",
    "        model_file = os.path.join(run_folder, '%02d_weights-improvement.h5' %(test_index))\n",
    "        history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(test_indeces)==1))\n",
    "        model.save(model_file)\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    if early_stopping:\n",
    "        plt.plot(history.history['val_loss'], label='val')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(run_folder, 'learning curve %02d.png') %(test_index))\n",
    "    plt.close(fig)\n",
    "\n",
    "    '''\n",
    "    save_name = '90%_30_20_0_4ep_'\n",
    "    save_file = os.path.join(results_folder, save_name+'.h5')\n",
    "\n",
    "    model1.save(save_file)\n",
    "    '''\n",
    "    test_output = model.predict(test_neuro)\n",
    "    \n",
    "    test_output_compiled.append(test_output)\n",
    "    test_spec_compiled.append(test_spec)\n",
    "    if early_stopping:\n",
    "        this_history = [history.history['loss'], history.history['val_loss']]\n",
    "    else:\n",
    "        this_history = [history.history['loss']]\n",
    "    history_compiled.append(this_history)\n",
    "    \n",
    "    print('Data recorded.')\n",
    "    \n",
    "    print('-'*50+'\\n')\n",
    "\n",
    "save_file = os.path.join(run_folder,save_name+'.p')\n",
    "pickle.dump([test_output_compiled, test_spec_compiled, history_compiled], open(save_file, 'w'))\n",
    "\n",
    "\n",
    "predicted_song_compiled, original_song_compiled, rmse = spp.sort_songs(test_output_compiled, test_spec_compiled,\n",
    "                                                                       num_songs, num_bins, divisor, \n",
    "                                                                       break_song=break_song, test_indeces=test_indeces)\n",
    "\n",
    "if slice_shuffle_pattern:   \n",
    "    sorted_predicted = list()\n",
    "    sorted_original = list()\n",
    "    for i in range(len(predicted_song_compiled)):\n",
    "        sorted_predicted.append(spp.reorder(predicted_song_compiled[i], generated_pattern))\n",
    "        sorted_original.append(spp.reorder(original_song_compiled[i], generated_pattern))\n",
    "\n",
    "    pickle.dump([sorted_predicted, sorted_original, rmse, history_compiled], open(save_file, 'w'))\n",
    "else:\n",
    "    pickle.dump([predicted_song_compiled, original_song_compiled, rmse, history_compiled], open(save_file, 'w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unshuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig_folder = os.path.join(bird_folder_save, 'day-2016-09-11', 'mel','2018_04_04_10_46_08')\n",
    "positions = np.load(os.path.join(fig_folder, 'positions.npy'))\n",
    "specs = np.load(os.path.join(fig_folder, 'specs.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recon_positions = list()\n",
    "for t in range(positions.shape[1]):\n",
    "    this_trial = list()\n",
    "    for c in range(positions.shape[0]):\n",
    "        this_trial.append(positions[c][t])\n",
    "    recon_positions.append(this_trial)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coeff = spp.eval_performance(specs, mode = 'self', matric = 'corr', output_all=True)\n",
    "print(np.mean(coeff))\n",
    "print(len(coeff))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matdic = {}\n",
    "for c in range(64):\n",
    "    i = 1\n",
    "    for j in range(positions[c].size):\n",
    "        positions[c][j] = positions[c][j].astype('double')\n",
    "    while not positions[c][0].size:\n",
    "        positions[c][0], positions[c][i] = positions[c][i], positions[c][0]\n",
    "        i += 1\n",
    "        if i==61:\n",
    "            positions[c][0] = np.array([0])\n",
    "    #np.savetxt(os.path.join(fig_folder, 'spikes.txt'), positions[c])\n",
    "    matdic['spikes%02d' %c] = positions[c]\n",
    "sio.savemat(os.path.join(fig_folder, 'spikes.mat'), matdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_specs = specs.transpose(2, 0, 1)\n",
    "print(new_specs.shape)\n",
    "print(new_specs[:,0,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norm_specs = spp.normalize_list(new_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def performance(freqs):\n",
    "    all_cc = list()\n",
    "    all_dist = list()\n",
    "    for freq in freqs:\n",
    "        cc = list()\n",
    "        dist = list()\n",
    "        for i in range(len(freq)):\n",
    "            j = i\n",
    "            while j<len(freq):\n",
    "                cc.append(np.corrcoef(freq[i], freq[j])[0, 1])\n",
    "                dist.append(np.sqrt(np.mean(np.square(freq[i]-freq[j]))))\n",
    "                j = j+1\n",
    "                print(j)\n",
    "        all_cc.append(np.mean(cc))\n",
    "        all_dist.append(np.mean(dist))\n",
    "    return all_cc, all_dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc, dist = performance(norm_specs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(PSTH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(cc)\n",
    "print(cc)\n",
    "print(np.mean(cc[3:-7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(dist)\n",
    "print(np.mean(dist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "this_slice = new_specs[16]\n",
    "plt.plot(this_slice[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_local_max(this_cluster, w):\n",
    "    #w is the delta t, window size to determine local max\n",
    "    max_vals = list()\n",
    "    max_positions = list()\n",
    "    max_binary = list()\n",
    "    for this_trial in this_cluster:\n",
    "        thresh = np.percentile(this_trial, 60)\n",
    "        threshed = list(this_trial*np.array([item>thresh for item in this_trial]))\n",
    "        '''\n",
    "        for i in np.arange(w, len(threshed)-w):\n",
    "            if threshed[i]<np.max(threshed[i-w:i+w+1]):\n",
    "                threshed[i]=0\n",
    "        '''\n",
    "        max_vals.append(threshed)\n",
    "        max_positions.append([i for i, x in enumerate(threshed) if x>0])\n",
    "        max_binary.append([t>0 for t in threshed])\n",
    "    \n",
    "    counts = np.sum(np.array(max_binary), axis = 0)\n",
    "    plt.figure(1)\n",
    "    plt.plot(counts)\n",
    "    print(type(w))\n",
    "    counts_thresh = len(this_cluster)*0.5\n",
    "    counts_threshed = counts*np.array([count>counts_thresh for count in counts])\n",
    "    max_counts = list()\n",
    "    lead = np.array(counts_threshed[:w])\n",
    "    for i in range(w):\n",
    "        max_counts+=[count*(count==np.max(lead)) for count in lead]\n",
    "    for i in np.arange(w, len(counts_threshed)-w):\n",
    "        if counts_threshed[i]<np.max(counts_threshed[i-w:i+w+1]):\n",
    "            max_counts.append(0)\n",
    "        else:\n",
    "            max_counts.append(counts_threshed[i])\n",
    "    plt.figure(2)\n",
    "    plt.plot(max_counts)\n",
    "    print(np.count_nonzero(max_counts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "find_local_max(this_slice, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(np.array(positions[0]).shape)\n",
    "print(positions[0].flatten)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spec_power = list()\n",
    "print(specs.shape)\n",
    "for spec in specs:\n",
    "    spec_power.append(np.sum(spec, axis=1))\n",
    "spec_power = np.array(spec_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_start = bin_size*num_lookbacks\n",
    "count_end = bin_size*(num_lookbacks+specs.shape[1])\n",
    "PSTH = list()\n",
    "for position in recon_positions:\n",
    "    flat_position = [time for trial in position for time in trial]\n",
    "    counts, _ = np.histogram(flat_position, bins=np.arange(count_start, count_end+1, bin_size))\n",
    "    PSTH.append(counts)\n",
    "PSTH = np.array(PSTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(spec_power.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PSTH.shape)\n",
    "#plt.matshow(PSTH)\n",
    "plt.plot(PSTH[62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warped_psth_cc=list()\n",
    "for i in range(len(PSTH)):\n",
    "    for j in range(i, len(PSTH)):\n",
    "        warped_psth_cc.append(np.corrcoef(PSTH[i], PSTH[j])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(warped_psth_cc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cc = list()\n",
    "for hist in PSTH:\n",
    "    this_cc = list()\n",
    "    for power in spec_power:\n",
    "        this_cc.append(np.corrcoef(power, hist)[0][1])\n",
    "    cc.append(np.mean(this_cc))\n",
    "print(cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(fig_folder, 'unshuffled_cc.npy'), cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shuffled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_fig_folder = os.path.join(bird_folder_save, 'day-2016-09-11', 'mel','2018_04_10_14_37_11')\n",
    "s_positions = np.load(os.path.join(s_fig_folder, 'positions.npy'))\n",
    "s_specs = np.load(os.path.join(s_fig_folder, 'specs.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_recon_positions = list()\n",
    "for t in range(s_positions.shape[1]):\n",
    "    this_trial = list()\n",
    "    for c in range(s_positions.shape[0]):\n",
    "        this_trial.append(s_positions[c][t])\n",
    "    s_recon_positions.append(this_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_spec_power = list()\n",
    "print(s_specs.shape)\n",
    "for spec in s_specs:\n",
    "    s_spec_power.append(np.sum(spec, axis=1))\n",
    "s_spec_power = np.array(s_spec_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_PSTH = list()\n",
    "for position in s_recon_positions:\n",
    "    flat_position = [time for trial in position for time in trial]\n",
    "    counts, _ = np.histogram(flat_position, bins=np.arange(count_start, count_end+1, bin_size))\n",
    "    s_PSTH.append(counts)\n",
    "s_PSTH = np.array(s_PSTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(PSTH[62])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_cc = list()\n",
    "for hist in s_PSTH:\n",
    "    this_cc = list()\n",
    "    for power in s_spec_power:\n",
    "        this_cc.append(np.corrcoef(power, hist)[0][1])\n",
    "    s_cc.append(np.mean(this_cc))\n",
    "print(s_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(fig_folder, 'shuffled_cc.npy'), s_cc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unwarped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_fig_folder = os.path.join(bird_folder_save, 'day-2016-09-11', 'mel','2018_04_10_14_56_53')\n",
    "u_positions = np.load(os.path.join(u_fig_folder, 'positions.npy'))\n",
    "u_specs = np.load(os.path.join(u_fig_folder, 'specs.npy'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_recon_positions = list()\n",
    "for t in range(u_positions.shape[1]):\n",
    "    this_trial = list()\n",
    "    for c in range(u_positions.shape[0]):\n",
    "        this_trial.append(u_positions[c][t])\n",
    "    u_recon_positions.append(this_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_spec_power = list()\n",
    "print(u_specs.shape)\n",
    "for spec in u_specs:\n",
    "    u_spec_power.append(np.sum(spec, axis=1))\n",
    "u_spec_power = np.array(u_spec_power)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_PSTH = list()\n",
    "for position in u_recon_positions:\n",
    "    flat_position = [time for trial in position for time in trial]\n",
    "    counts, _ = np.histogram(flat_position, bins=np.arange(count_start, count_end+1, bin_size))\n",
    "    u_PSTH.append(counts)\n",
    "u_PSTH = np.array(u_PSTH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(u_PSTH.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unwarped_psth_cc=list()\n",
    "warped_unwarped_cc = list()\n",
    "for i in range(len(u_PSTH)):\n",
    "    for j in range(i, len(u_PSTH)):\n",
    "        unwarped_psth_cc.append(np.corrcoef(u_PSTH[i], u_PSTH[j])[0][1])\n",
    "    warped_unwarped_cc.append(np.corrcoef(u_PSTH[i], PSTH[i])[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_PSTH[0], PSTH[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(sum(PSTH[30][:-1]))\n",
    "print(sum(u_PSTH[30]))\n",
    "print(np.corrcoef(PSTH[30][:-1], PSTH[30][1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(PSTH)-1):\n",
    "    plt.figure(1, figsize=(10,6))\n",
    "    plt.title('Unwarped PSTH')\n",
    "    plt.plot(u_PSTH[i][:-1])\n",
    "    plt.savefig(os.path.join(fig_folder, 'iterate_figs', '%02d unwarped PSTH.pdf' %(i)))\n",
    "    plt.close(1)\n",
    "    plt.figure(2, figsize=(10,6))\n",
    "    plt.title('Warped PSTH')\n",
    "    plt.plot(PSTH[i][:-1])\n",
    "    plt.savefig(os.path.join(fig_folder, 'iterate_figs', '%02d warped PSTH.pdf' %(i)))\n",
    "    plt.close(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_cc = list()\n",
    "for hist in u_PSTH:\n",
    "    this_cc = list()\n",
    "    for power in u_spec_power:\n",
    "        this_cc.append(np.corrcoef(power, hist)[0][1])\n",
    "    u_cc.append(np.mean(this_cc))\n",
    "print(u_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_real = [item for item in u_cc if not np.isnan(item)]\n",
    "np.mean(u_real)\n",
    "np.std(u_real)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save(os.path.join(fig_folder, 'unwarped_cc.npy'), u_cc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize= (10, 6))\n",
    "plt.plot(cc, label ='unshuffled', color = 'r')\n",
    "plt.plot(s_cc, label = 'shuffled', color = 'b')\n",
    "plt.plot(u_cc, label = 'unwarped', color = 'y')\n",
    "plt.xlabel('Trial Number')\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Correlation Coefficient between Spectral Power and Spike Counts across Trials')\n",
    "plt.legend()\n",
    "plt.savefig(os.path.join(fig_folder, 'cc across trails.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_real = [item for item in u_cc if not np.isnan(item)]\n",
    "real = [item for item in cc if not np.isnan(item)]\n",
    "s_real = [item for item in s_cc if not np.isnan(item)]\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([u_real, real, s_real], labels = ['unwarped', 'unshuffled', 'shuffled'])\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Correlation Coefficient between Spectral Power and Spike Counts')\n",
    "plt.savefig(os.path.join(fig_folder, 'cc across groups.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.boxplot([warped_unwarped_cc, warped_psth_cc, unwarped_psth_cc], labels = ['warped vs unwarped', 'within warped', 'within unwarped'])\n",
    "plt.ylabel('Correlation Coefficient')\n",
    "plt.title('Spike Count Correlation Coefficients')\n",
    "plt.savefig(os.path.join(fig_folder, 'cc spike counts across groups.pdf'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
