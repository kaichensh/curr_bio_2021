{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FFNN network to predict mel spectrogram slices from HVC spikes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "gpus = [3]\n",
    "os.environ['CUDA_VISIBLE_DEVICES']=','.join([str(i) for i in gpus])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylab\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import math\n",
    "import socket\n",
    "import sys\n",
    "import h5py\n",
    "import keras\n",
    "import random\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential, load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import LSTM, Activation, advanced_activations\n",
    "from keras.regularizers import l2\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pickle\n",
    "import spec_processing as spp\n",
    "%matplotlib inline\n",
    "from __future__ import division\n",
    "from scipy.signal import spectrogram\n",
    "import scipy.interpolate\n",
    "import logging\n",
    "import copy\n",
    "import glob\n",
    "import shutil\n",
    "import os\n",
    "import datetime\n",
    "from random import shuffle\n",
    "import IPython.display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#date and bird of data used\n",
    "bird_id = 'z007'\n",
    "model_name = 'spectrogram prediction model/mel'\n",
    "\n",
    "#locate source folder\n",
    "bird_folder_data, bird_folder_save, repos_folder, results_folder = spp.locate_folders(bird_id, model_name)\n",
    "    \n",
    "print('Bird data located at:\\n'+bird_folder_data+'\\n')\n",
    "print('Results will be saved at:\\n'+results_folder+'\\n')\n",
    "\n",
    "output_file = os.path.join(results_folder, 'mel_spec_predicted.dat')\n",
    "\n",
    "sys.path.append(os.path.join(repos_folder, 'swissknife'))\n",
    "\n",
    "from dynamo import finch as zf\n",
    "from dynamo import takens as tk\n",
    "from bci.core import expstruct as es\n",
    "from bci import synthetic as syn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neural_kwik_name = 'experiment.kwik'\n",
    "song_kwik_name = 'experiment.sng.kwe'\n",
    "song_length = 20480\n",
    "bin_size = 128\n",
    "##\n",
    "num_clusters = 64\n",
    "num_lookbacks = 10\n",
    "\n",
    "start_extend_bins = 0\n",
    "end_extend_bins = 0\n",
    "n_mel_freq = 64\n",
    "slice_shuffle_pattern = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reload(spp)\n",
    "datasets = spp.make_datasets_finch_ffnn(neural_kwik_name, song_kwik_name, song_length, bin_size, num_lookbacks, num_clusters,\n",
    "                                        bird_id, model_name, start_extend_bins=start_extend_bins, end_extend_bins=end_extend_bins,\n",
    "                                        specify_subdir = 'day-2016-09-10', n_mel_freq_components = n_mel_freq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(datasets))\n",
    "print(len(datasets[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test_percentage(dataset, num_bins, percentage, break_song=False, cross_valid=False):\n",
    "    '''\n",
    "    Given a dataset, divide the entire set into training and testing sets corresponding to the specified percentage\n",
    "    dataset: list, shape (n_songs * n_bins_per_song, 2) where 2 corresponds to input and target arrays\n",
    "    num_bins: int, number of bins per song\n",
    "    percentage: int, percentage of dataset to be used as training\n",
    "    break_song: bool, whether to break songs during training (i.e. piece-wise training vs song-wise training)\n",
    "    cross_valid: bool, whether to generate cross validation dataset\n",
    "    '''\n",
    "    num_songs = int(len(dataset)/int(num_bins))\n",
    "    \n",
    "    if break_song:\n",
    "        \n",
    "        #break songs into roughly equal parts\n",
    "        num_bins_each_part = spp.divide_parts(num_bins, divisor)\n",
    "        start_indeces, end_indeces = spp.find_start_end_index(num_bins_each_part)\n",
    "        \n",
    "        num_current_test = num_bins_each_part[test_index]\n",
    "        num_current_valid = num_bins_each_part[valid_index]*cross_valid\n",
    "        num_current_train = num_bins-num_current_test-num_current_valid\n",
    "        \n",
    "        #print('Breaking up each song into '+str(num_current_train)+' training points and '+str(num_current_test)+' testing points.')\n",
    "        #print('Currently testing on part No.'+str(test_index+1)+' of the '+ str(divisor)+' parts.')\n",
    "        \n",
    "        train_list = list()\n",
    "        test_list = list()\n",
    "        valid_list = list()\n",
    "        temp_test = list()\n",
    "        temp_train = list()\n",
    "        temp_valid = list()\n",
    "        \n",
    "        for i in range(num_songs):\n",
    "            this_song = dataset[i*num_bins:(i+1)*num_bins]\n",
    "            bins_by_part = list()\n",
    "            for j in range(len(num_bins_each_part)):\n",
    "                start_index = start_indeces[j]\n",
    "                end_index = end_indeces[j]\n",
    "                if test_index == j:\n",
    "                    temp_test = temp_test+this_song[start_index:end_index]\n",
    "                elif valid_index == j and cross_valid:\n",
    "                    temp_valid = temp_valid+this_song[start_index:end_index]\n",
    "                else:\n",
    "                    temp_train = temp_train+this_song[start_index:end_index]\n",
    "                    \n",
    "        #separate neural and song data\n",
    "        for i in range(len(dataset[0])):            \n",
    "            train_list.append([train_set[i] for train_set in temp_train])\n",
    "            test_list.append([test_set[i] for test_set in temp_test])\n",
    "            valid_list.append([valid_set[i] for valid_set in temp_valid])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        num_songs_each_part = num_songs\n",
    "        num_train = int(round(num_songs*percentage/100.0))*num_bins\n",
    "        num_test = len(dataset)-num_train\n",
    "    \n",
    "        print('Not breaking up songs. Percentage value: '+str(percentage)+'. There are '+str(num_train)+' training sets, and ' +str(num_test)+' testing sets.\\n')\n",
    "    \n",
    "        train_list = list()\n",
    "        test_list = list()\n",
    "        valid_list = list()\n",
    "        temp_test = dataset[num_train:]\n",
    "        temp_train = dataset[:num_train]\n",
    "        \n",
    "        for i in range(len(dataset[0])):\n",
    "            train_list.append([train_set[i] for train_set in temp_train])\n",
    "            test_list.append([test_set[i] for test_set in temp_test])\n",
    "        \n",
    "    return train_list, test_list, valid_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_train_test(dataset, num_bins, divisor=4, break_song=False, test_index=3, cross_valid=False):\n",
    "    '''\n",
    "    Given a dataset, divide the entire set a certain number of parts, use one part for testing and the rest for training\n",
    "    dataset: list, shape (n_songs * n_bins_per_song, 2) where 2 corresponds to input and target arrays\n",
    "    num_bins: int, number of bins per song\n",
    "    divisor: int, number of parts the entire dataset will be divded into\n",
    "    break_song: bool, whether to break songs during training (i.e. piece-wise training vs song-wise training)\n",
    "    test_index: within the divided parts, which part is used for testing\n",
    "    cross_valid: bool, whether to generate cross validation dataset\n",
    "    '''\n",
    "    \n",
    "    if test_index>divisor-1 or type(test_index)!=int:\n",
    "        raise ValueError('test_index should be an integer less than or equal to 3.')\n",
    "    if type(divisor)!=int or divisor<1:\n",
    "        raise ValueError('divisor should be an integer greater than or equal to 1.')\n",
    "       \n",
    "    valid_index = test_index-1\n",
    "    \n",
    "    if valid_index<0:\n",
    "        valid_index = divisor-1\n",
    "    \n",
    "    num_songs = int(len(dataset)/int(num_bins))\n",
    "    \n",
    "    #print('Total number of songs loaded:'+str(num_songs))\n",
    "    \n",
    "    if break_song:\n",
    "        \n",
    "        #break songs into roughly equal parts\n",
    "        num_bins_each_part = spp.divide_parts(num_bins, divisor)\n",
    "        start_indeces, end_indeces = spp.find_start_end_index(num_bins_each_part)\n",
    "        \n",
    "        num_current_test = num_bins_each_part[test_index]\n",
    "        num_current_valid = num_bins_each_part[valid_index]*cross_valid\n",
    "        num_current_train = num_bins-num_current_test-num_current_valid\n",
    "        \n",
    "        #print('Breaking up each song into '+str(num_current_train)+' training points and '+str(num_current_test)+' testing points.')\n",
    "        #print('Currently testing on part No.'+str(test_index+1)+' of the '+ str(divisor)+' parts.')\n",
    "        \n",
    "        train_list = list()\n",
    "        test_list = list()\n",
    "        valid_list = list()\n",
    "        temp_test = list()\n",
    "        temp_train = list()\n",
    "        temp_valid = list()\n",
    "        \n",
    "        for i in range(num_songs):\n",
    "            this_song = dataset[i*num_bins:(i+1)*num_bins]\n",
    "            bins_by_part = list()\n",
    "            for j in range(len(num_bins_each_part)):\n",
    "                start_index = start_indeces[j]\n",
    "                end_index = end_indeces[j]\n",
    "                if test_index == j:\n",
    "                    temp_test = temp_test+this_song[start_index:end_index]\n",
    "                elif valid_index == j and cross_valid:\n",
    "                    temp_valid = temp_valid+this_song[start_index:end_index]\n",
    "                else:\n",
    "                    temp_train = temp_train+this_song[start_index:end_index]\n",
    "                    \n",
    "        #separate neural and song data\n",
    "        for i in range(len(dataset[0])):            \n",
    "            train_list.append([train_set[i] for train_set in temp_train])\n",
    "            test_list.append([test_set[i] for test_set in temp_test])\n",
    "            valid_list.append([valid_set[i] for valid_set in temp_valid])\n",
    "        \n",
    "        \n",
    "    else:\n",
    "        num_songs_each_part = spp.divide_parts(num_songs, divisor)\n",
    "        num_test = num_songs_each_part[test_index]*num_bins\n",
    "        num_valid = num_songs_each_part[valid_index]*num_bins\n",
    "        num_train = len(dataset)-num_test-num_valid*cross_valid\n",
    "        \n",
    "        start_indeces, end_indeces = spp.find_start_end_index(num_songs_each_part)\n",
    "        \n",
    "    \n",
    "        print('Not breaking up songs. There are '+str(num_train)+' training sets, and '+str(num_test)+' testing sets.\\n')\n",
    "    \n",
    "        train_list = list()\n",
    "        test_list = list()\n",
    "        valid_list = list()\n",
    "        temp_test = list()\n",
    "        temp_train = list()\n",
    "        temp_valid = list()\n",
    "        \n",
    "        #separate neural and song data\n",
    "        for j in range(divisor):\n",
    "            start_index = start_indeces[j]*num_bins\n",
    "            end_index = end_indeces[j]*num_bins\n",
    "            \n",
    "            if test_index == j:\n",
    "                temp_test = temp_test+dataset[start_index:end_index]\n",
    "            elif valid_index == j and cross_valid:\n",
    "                temp_valid = temp_valid+dataset[start_index:end_index]\n",
    "            else:\n",
    "                temp_train = temp_train+dataset[start_index:end_index]\n",
    "                \n",
    "        for i in range(len(dataset[0])):\n",
    "            train_list.append([train_set[i] for train_set in temp_train])\n",
    "            test_list.append([test_set[i] for test_set in temp_test])\n",
    "            valid_list.append([valid_set[i] for valid_set in temp_valid])\n",
    "        \n",
    "    return train_list, test_list, valid_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "break_song = False\n",
    "num_bins = song_length//bin_size+(end_extend_bins-start_extend_bins)*16-2\n",
    "print(num_bins)\n",
    "divisor= 10\n",
    "l2_val = 0.001\n",
    "dropout_val = 0.2\n",
    "cross_valid = False\n",
    "num_neurons = [20, 30]\n",
    "num_ep = 1000\n",
    "patience = 50\n",
    "early_stopping = True\n",
    "valid_split = 0.1\n",
    "test4valid = False\n",
    "batch_size = 10\n",
    "num_songs = len(datasets)//num_bins\n",
    "if len(datasets)%num_bins:\n",
    "    raise ValueError('number of songs is not integer')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_indeces = spp.assign_test_indeces(divisor, break_song=break_song)\n",
    "#del model\n",
    "#test_indeces = [9]\n",
    "test_indeces = range(divisor)\n",
    "print(test_indeces)\n",
    "\n",
    "num_bins_per_part = spp.divide_parts(num_bins, divisor)\n",
    "start_index, end_index = spp.find_start_end_index(num_bins_per_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "percentages = range(80,100,10)\n",
    "print(percentages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build and train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_output_compiled = list()\n",
    "test_spec_compiled = list()\n",
    "history_compiled = list()\n",
    "\n",
    "save_name = '[%d_%d_%02d]_%02d_%02d_[%s_%.2f]_[%03d_%03d]_%.3f_%03dep_%s' %(start_extend_bins, end_extend_bins, num_lookbacks, divisor, \n",
    "                                                            test_indeces[0], cross_valid, valid_split, num_neurons[0], \n",
    "                                                            num_neurons[1], l2_val, num_ep, break_song)\n",
    "#save_name example:[2_2_10]_10_7_[False_0.10]_[20_30]_0_2ep_True\n",
    "run_folder = os.path.join(results_folder, '{:%Y_%m_%d_%H_%M_%S}'.format(datetime.datetime.now()))\n",
    "if not os.path.exists(run_folder):\n",
    "    os.makedirs(run_folder)\n",
    "\n",
    "par_dict = {'song_length':song_length,\n",
    "             'bin_size':bin_size, \n",
    "             'num_clusters':num_clusters, \n",
    "             'num_lookbacks':num_lookbacks, \n",
    "             'start_extend_bins':start_extend_bins, \n",
    "             'end_extend_bins':end_extend_bins, \n",
    "             'n_mel_freq':n_mel_freq, \n",
    "             'slice_shuffle_pattern':slice_shuffle_pattern,\n",
    "            'break_song':break_song,\n",
    "            'divisor':divisor,\n",
    "            'l2_val':l2_val,\n",
    "            'dropout_val':dropout_val,\n",
    "            'cross_valid':cross_valid,\n",
    "            'num_neurons':num_neurons,\n",
    "            'num_ep':num_ep,\n",
    "            'early_stopping':early_stopping,\n",
    "            'valid_split':valid_split,\n",
    "            'test4valid':test4valid,\n",
    "            'batch_size':batch_size,\n",
    "            'test_indeces':test_indeces}\n",
    "\n",
    "dict_file = os.path.join(run_folder, 'parameters.npy')\n",
    "np.save(dict_file, par_dict)\n",
    "    \n",
    "if early_stopping and not (cross_valid or valid_split or test4valid):\n",
    "    raise ValueError('if early stopping, you need to have validation sets.')\n",
    "    \n",
    "for test_index in test_indeces:\n",
    "    \n",
    "    if break_song:\n",
    "        print('Starting preparing for No.'+ str(test_index+1)+ ' of '+str(divisor)+' parts.')\n",
    "        \n",
    "    train_list, test_list, valid_list = make_train_test(datasets, num_bins, divisor=divisor, \n",
    "                                                        break_song=break_song, test_index=test_index, \n",
    "                                                        cross_valid=cross_valid)\n",
    "    #both lists [neural data, on/off, beta, alpha]\n",
    "    train_neuro = np.array(train_list[0])\n",
    "    test_neuro = np.array(test_list[0])    \n",
    "\n",
    "    train_spec = np.array(train_list[1])\n",
    "    test_spec = np.array(test_list[1])\n",
    "\n",
    "    #train_neuro = scaler.fit_transform(train_neuro)\n",
    "    #test_neuro = scaler.fit_transform(test_neuro)\\\n",
    "\n",
    "    \n",
    "    if cross_valid:\n",
    "        valid_neuro = np.array(valid_list[0])\n",
    "        valid_spec = np.array(valid_list[1])\n",
    "    \n",
    "    #train_neuro = np.reshape(train_neuro, (train_neuro.shape[0], 1, train_neuro.shape[1]))\n",
    "    #test_neuro = np.reshape(test_neuro, (test_neuro.shape[0], 1, test_neuro.shape[1]))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #num_lookback = len(datasets[0][0])//num_channels\n",
    "    #print('The number of lookbacks in this model is '+ str(num_lookbacks) + '\\n')\n",
    "    num_bins_per_part = spp.divide_parts(num_bins, divisor)\n",
    "    \n",
    "    model.add(Dense(num_neurons[0], input_dim = num_clusters*num_lookbacks, W_regularizer=l2(l2_val)))\n",
    "    #model1.add(LSTM(10, input_dim = num_lookback*num_channels,return_sequences=True))\n",
    "    model.add(Dropout(dropout_val))\n",
    "    model.add(Dense(num_neurons[1], W_regularizer=l2(l2_val)))\n",
    "    model.add(Dropout(dropout_val))\n",
    "    model.add(Dense(len(datasets[0][1]), W_regularizer=l2(l2_val)))\n",
    "\n",
    "    print('Model building finished.')\n",
    "    \n",
    "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "    \n",
    "    current_ep_count=0\n",
    "    \n",
    "    if early_stopping:\n",
    "        model_file = os.path.join(run_folder, '%02d_weights-improvement.h5' %(test_index))\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=patience, verbose=1),\n",
    "                     ModelCheckpoint(filepath=model_file, monitor='val_loss', save_best_only=True, verbose=0)\n",
    "                    ]\n",
    "        if valid_split:\n",
    "            history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(test_indeces)==1), \n",
    "                      validation_split=valid_split, callbacks=callbacks)\n",
    "        elif test4valid:\n",
    "            history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(test_indeces)==1), \n",
    "                      validation_data=(test_neuro, test_spec), callbacks=callbacks)\n",
    "        else:\n",
    "            history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(test_indeces)==1), \n",
    "                      validation_data=(valid_neuro, valid_spec), callbacks=callbacks)\n",
    "        del model\n",
    "        model = load_model(model_file)\n",
    "    \n",
    "    else:\n",
    "        model_file = os.path.join(run_folder, '%02d_weights-improvement.h5' %(test_index))\n",
    "        history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(test_indeces)==1))\n",
    "        model.save(model_file)\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    if early_stopping:\n",
    "        plt.plot(history.history['val_loss'], label='val')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(run_folder, 'learning curve %02d.png') %(test_index))\n",
    "    plt.close(fig)\n",
    "\n",
    "    '''\n",
    "    save_name = '90%_30_20_0_4ep_'\n",
    "    save_file = os.path.join(results_folder, save_name+'.h5')\n",
    "\n",
    "    model1.save(save_file)\n",
    "    '''\n",
    "    test_output = model.predict(test_neuro)\n",
    "    \n",
    "    test_output_compiled.append(test_output)\n",
    "    test_spec_compiled.append(test_spec)\n",
    "    if early_stopping:\n",
    "        this_history = [history.history['loss'], history.history['val_loss']]\n",
    "    else:\n",
    "        this_history = [history.history['loss']]\n",
    "    history_compiled.append(this_history)\n",
    "    \n",
    "    print('Data recorded.')\n",
    "    \n",
    "    print('-'*50+'\\n')\n",
    "\n",
    "save_file = os.path.join(run_folder,save_name+'.p')\n",
    "pickle.dump([test_output_compiled, test_spec_compiled, history_compiled], open(save_file, 'w'))\n",
    "\n",
    "predicted_song_compiled, original_song_compiled, rmse = spp.sort_songs(test_output_compiled, test_spec_compiled,\n",
    "                                                                       num_songs, num_bins, divisor, \n",
    "                                                                       break_song=break_song, test_indeces=test_indeces)\n",
    "\n",
    "pickle.dump([predicted_song_compiled, original_song_compiled, rmse, history_compiled], open(save_file, 'w'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percentage Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_compiled = list()\n",
    "rmse_compiled = list()\n",
    "\n",
    "save_name = '[%d_%d_%02d]_%02d_[%s_%.2f]_[%03d_%03d]_%.3f_%03dep_%s' %(start_extend_bins, end_extend_bins, num_lookbacks, percentages[0], \n",
    "                                                            cross_valid, valid_split, num_neurons[0], \n",
    "                                                            num_neurons[1], l2_val, num_ep, break_song)\n",
    "#save_name example:[2_2_10]_10_7_[False_0.10]_[20_30]_0_2ep_True\n",
    "run_folder = os.path.join(results_folder, '{:%Y_%m_%d_%H_%M_%S}'.format(datetime.datetime.now()))\n",
    "if not os.path.exists(run_folder):\n",
    "    os.makedirs(run_folder)\n",
    "\n",
    "par_dict = {'song_length':song_length,\n",
    "             'bin_size':bin_size, \n",
    "             'num_clusters':num_clusters, \n",
    "             'num_lookbacks':num_lookbacks, \n",
    "             'start_extend_bins':start_extend_bins, \n",
    "             'end_extend_bins':end_extend_bins, \n",
    "             'n_mel_freq':n_mel_freq, \n",
    "             'slice_shuffle_pattern':slice_shuffle_pattern,\n",
    "            'break_song':break_song,\n",
    "            'percentages':percentages,\n",
    "            'l2_val':l2_val,\n",
    "            'dropout_val':dropout_val,\n",
    "            'cross_valid':cross_valid,\n",
    "            'num_neurons':num_neurons,\n",
    "            'num_ep':num_ep,\n",
    "            'early_stopping':early_stopping,\n",
    "            'valid_split':valid_split,\n",
    "            'test4valid':test4valid,\n",
    "            'batch_size':batch_size}\n",
    "\n",
    "dict_file = os.path.join(run_folder, 'parameters.npy')\n",
    "np.save(dict_file, par_dict)\n",
    "    \n",
    "if early_stopping and not (cross_valid or valid_split or test4valid):\n",
    "    raise ValueError('if early stopping, you need to have validation sets.')\n",
    "    \n",
    "for percentage in percentages:\n",
    "        \n",
    "    train_list, test_list, valid_list = make_train_test_percentage(datasets, num_bins, percentage, \n",
    "                                                        break_song=break_song, \n",
    "                                                        cross_valid=cross_valid)\n",
    "    \n",
    "    train_neuro = np.array(train_list[0])\n",
    "    test_neuro = np.array(test_list[0])    \n",
    "\n",
    "    train_spec = np.array(train_list[1])\n",
    "    test_spec = np.array(test_list[1])\n",
    "\n",
    "    #train_neuro = scaler.fit_transform(train_neuro)\n",
    "    #test_neuro = scaler.fit_transform(test_neuro)\\\n",
    "\n",
    "    \n",
    "    if cross_valid:\n",
    "        valid_neuro = np.array(valid_list[0])\n",
    "        valid_spec = np.array(valid_list[1])\n",
    "    \n",
    "    #train_neuro = np.reshape(train_neuro, (train_neuro.shape[0], 1, train_neuro.shape[1]))\n",
    "    #test_neuro = np.reshape(test_neuro, (test_neuro.shape[0], 1, test_neuro.shape[1]))\n",
    "\n",
    "    model = Sequential()\n",
    "\n",
    "    #num_lookback = len(datasets[0][0])//num_channels\n",
    "    #print('The number of lookbacks in this model is '+ str(num_lookbacks) + '\\n')\n",
    "    num_bins_per_part = spp.divide_parts(num_bins, divisor)\n",
    "    \n",
    "    model.add(Dense(num_neurons[0], input_dim = num_clusters*num_lookbacks, W_regularizer=l2(l2_val)))\n",
    "    #model1.add(LSTM(10, input_dim = num_lookback*num_channels,return_sequences=True))\n",
    "    model.add(Dropout(dropout_val))\n",
    "    model.add(Dense(num_neurons[1], W_regularizer=l2(l2_val)))\n",
    "    model.add(Dropout(dropout_val))\n",
    "    model.add(Dense(len(datasets[0][1]), W_regularizer=l2(l2_val)))\n",
    "\n",
    "    print('Model building finished.')\n",
    "    \n",
    "    model.compile(loss = 'mean_squared_error', optimizer = 'adam')\n",
    "    \n",
    "    current_ep_count=0\n",
    "    \n",
    "    if early_stopping:\n",
    "        model_file = os.path.join(run_folder, '%02d_weights-improvement.h5' %(percentage))\n",
    "        callbacks = [EarlyStopping(monitor='val_loss', patience=patience, verbose=1),\n",
    "                     ModelCheckpoint(filepath=model_file, monitor='val_loss', save_best_only=True, verbose=0)\n",
    "                    ]\n",
    "        if valid_split:\n",
    "            history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(percentages)==1), \n",
    "                      validation_split=valid_split, callbacks=callbacks)\n",
    "        elif test4valid:\n",
    "            history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(percentages)==1), \n",
    "                      validation_data=(test_neuro, test_spec), callbacks=callbacks)\n",
    "        else:\n",
    "            history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(percentages)==1), \n",
    "                      validation_data=(valid_neuro, valid_spec), callbacks=callbacks)\n",
    "        del model\n",
    "        model = load_model(model_file)\n",
    "    \n",
    "    else:\n",
    "        model_file = os.path.join(run_folder, '%02d_weights-improvement.h5' %(percentage))\n",
    "        history = model.fit(train_neuro, train_spec, nb_epoch=num_ep, batch_size=batch_size, verbose=int(len(percentages)==1))\n",
    "        model.save(model_file)\n",
    "        \n",
    "    fig = plt.figure()\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    if early_stopping:\n",
    "        plt.plot(history.history['val_loss'], label='val')\n",
    "    plt.legend()\n",
    "    plt.savefig(os.path.join(run_folder, 'learning curve %02d.png') %(percentage))\n",
    "    plt.close(fig)\n",
    "\n",
    "    test_output = model.predict(test_neuro)\n",
    "    rmse = [np.sqrt(np.mean(np.square(np.array(output)-np.array(original)))) for output, original in \n",
    "            zip(test_output, test_spec)]\n",
    "    \n",
    "    save_file = os.path.join(run_folder,'%d.p' %(percentage))\n",
    "    pickle.dump([test_output, test_spec, rmse], open(save_file, 'w'))\n",
    "    \n",
    "    if early_stopping:\n",
    "        this_history = [history.history['loss'], history.history['val_loss']]\n",
    "    else:\n",
    "        this_history = [history.history['loss']]\n",
    "    history_compiled.append(this_history)\n",
    "    rmse_compiled.append(np.mean(rmse))\n",
    "    \n",
    "    print('Data recorded.')\n",
    "    \n",
    "    print('-'*50+'\\n')\n",
    "\n",
    "save_file = os.path.join(run_folder,'stats compiled'+'.p')\n",
    "pickle.dump([rmse_compiled, history_compiled], open(save_file, 'w'))\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(percentages, rmse_compiled)\n",
    "plt.xlabel('Percentage of Training')\n",
    "plt.ylabel('RMSE')\n",
    "plt.savefig(os.path.join(run_folder, 'all_rmse.png'))\n",
    "plt.close(fig)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
